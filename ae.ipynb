{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Modified by M. Romero.\n",
    "    Original: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc(\"font\", size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "targets = digits.target\n",
    "data = digits.data\n",
    "classes = digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 70 %\n",
      "val 10 %\n",
      "test 20 %\n"
     ]
    }
   ],
   "source": [
    "# train-validation-test split\n",
    "nsamples, ndim = data.shape\n",
    "nlabels = len(classes)\n",
    "indexes = np.arange(nsamples)\n",
    "train_idx, test_idx = train_test_split(indexes, test_size=0.2)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1/0.8)\n",
    "\n",
    "print(\"train {:.0f} %\".format(len(train_idx)/nsamples*100))\n",
    "print(\"val {:.0f} %\".format(len(val_idx)/nsamples*100))\n",
    "print(\"test {:.0f} %\".format(len(test_idx)/nsamples*100))\n",
    "\n",
    "x_train = data[train_idx]\n",
    "x_val = data[val_idx]\n",
    "x_test = data[test_idx]\n",
    "\n",
    "y_train = targets[train_idx]\n",
    "y_val = targets[val_idx]\n",
    "y_test = targets[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler() # (x - mu)/std, (mu, std) = (0, 1)\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de tensores y dispositivo de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "device = torch.device(\"cuda:0\" if cuda and torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float)\n",
    "x_val = torch.tensor(x_val, device=device, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.long) # ojo que al utilizar CrossEntropyLoss \n",
    "y_val = torch.tensor(y_val, device=device, dtype=torch.long)     # la entrada debe ser tipo long\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construccion de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, ninput, nhidden, nout, bn=False, do=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(ninput, nhidden))\n",
    "        layers.append(torch.nn.BatchNorm1d(nhidden)) if bn else 0\n",
    "        layers.append(torch.nn.Dropout(0.5)) if do else 0\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(nhidden, nout))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    \n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, nin, nl, bn=False, do=False):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc1 = torch.nn.Linear(nin, 32)\n",
    "        self.enc2 = torch.nn.Linear(32, nl)\n",
    "        self.dec1 = torch.nn.Linear(32, nin)\n",
    "        self.dec2 = torch.nn.Linear(nl, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        e1 = self.relu(self.enc1(x))\n",
    "        return self.enc2(e1)\n",
    "    \n",
    "    def decode(self, u):\n",
    "        d1 = self.relu(self.dec2(u))\n",
    "        return self.sig(self.dec1(d1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.encode(x)\n",
    "        r = self.decode(l)\n",
    "        return l, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decalaracion de hiper parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "wd = 0.\n",
    "bs = 10\n",
    "neurons = 100\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = torch.nn.CrossEntropyLoss()\n",
    "bcel = torch.nn.BCELoss()\n",
    "msel = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_my_model(epochs, model, optimizer, loss_function, trainloader, valloader, testloader):\n",
    "    losses = np.zeros((epochs, 3))\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        test_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            x_in, y_in = batch\n",
    "            _, x_out = model(x_in)\n",
    "            loss = loss_function(x_out, x_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= i + 1\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valloader):\n",
    "                x_in, y_in = batch\n",
    "                _, x_out = model(x_in)\n",
    "                loss = loss_function(x_out, x_in)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= i + 1\n",
    "            for i, batch in enumerate(testloader):\n",
    "                x_in, y_in = batch\n",
    "                _, x_out = model(x_in)\n",
    "                loss = loss_function(x_out, x_in)\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= i + 1\n",
    "        losses[epoch] = [train_loss, val_loss, test_loss]\n",
    "        print(\"Epoch {} Train loss {:.3f} Val loss {:.3f} Test loss {:.3f}\".format(epoch, train_loss, val_loss, test_loss))\n",
    "        if val_loss < best_val_loss:\n",
    "            print(\"Saving\")\n",
    "            torch.save(model.state_dict(), \"models/ae.pth\")\n",
    "            best_val_loss = val_loss\n",
    "    return losses\n",
    "\n",
    "\n",
    "def plot_my_loss(loss, title, ylabel=\"mean cross entropy\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(loss[:, 0], color=\"navy\", label=\"train\")\n",
    "    plt.plot(loss[:, 1], color=\"green\", label=\"val\")\n",
    "    plt.plot(loss[:, 2], color=\"red\", label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 2])\n",
    "    return\n",
    "\n",
    "\n",
    "def eval_my_model(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xl, x_out = model(test_data)\n",
    "    return xl.cpu().numpy(), x_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train loss 0.686 Val loss 0.673 Test loss 0.673\n",
      "Saving\n",
      "Epoch 1 Train loss 0.648 Val loss 0.615 Test loss 0.614\n",
      "Saving\n",
      "Epoch 2 Train loss 0.572 Val loss 0.528 Test loss 0.526\n",
      "Saving\n",
      "Epoch 3 Train loss 0.497 Val loss 0.479 Test loss 0.479\n",
      "Saving\n",
      "Epoch 4 Train loss 0.488 Val loss 0.493 Test loss 0.496\n",
      "Epoch 5 Train loss 0.488 Val loss 0.472 Test loss 0.476\n",
      "Saving\n",
      "Epoch 6 Train loss 0.458 Val loss 0.447 Test loss 0.451\n",
      "Saving\n",
      "Epoch 7 Train loss 0.447 Val loss 0.449 Test loss 0.450\n",
      "Epoch 8 Train loss 0.453 Val loss 0.456 Test loss 0.456\n",
      "Epoch 9 Train loss 0.458 Val loss 0.458 Test loss 0.457\n",
      "Epoch 10 Train loss 0.455 Val loss 0.450 Test loss 0.450\n",
      "Epoch 11 Train loss 0.447 Val loss 0.443 Test loss 0.443\n",
      "Saving\n",
      "Epoch 12 Train loss 0.439 Val loss 0.435 Test loss 0.434\n",
      "Saving\n",
      "Epoch 13 Train loss 0.429 Val loss 0.428 Test loss 0.426\n",
      "Saving\n",
      "Epoch 14 Train loss 0.425 Val loss 0.427 Test loss 0.425\n",
      "Saving\n",
      "Epoch 15 Train loss 0.422 Val loss 0.425 Test loss 0.424\n",
      "Saving\n",
      "Epoch 16 Train loss 0.422 Val loss 0.427 Test loss 0.426\n",
      "Epoch 17 Train loss 0.419 Val loss 0.422 Test loss 0.421\n",
      "Saving\n",
      "Epoch 18 Train loss 0.419 Val loss 0.423 Test loss 0.423\n",
      "Epoch 19 Train loss 0.418 Val loss 0.423 Test loss 0.423\n",
      "Epoch 20 Train loss 0.421 Val loss 0.422 Test loss 0.423\n",
      "Saving\n",
      "Epoch 21 Train loss 0.419 Val loss 0.424 Test loss 0.425\n",
      "Epoch 22 Train loss 0.420 Val loss 0.418 Test loss 0.419\n",
      "Saving\n",
      "Epoch 23 Train loss 0.418 Val loss 0.422 Test loss 0.424\n",
      "Epoch 24 Train loss 0.418 Val loss 0.421 Test loss 0.422\n",
      "Epoch 25 Train loss 0.422 Val loss 0.422 Test loss 0.423\n",
      "Epoch 26 Train loss 0.418 Val loss 0.423 Test loss 0.423\n",
      "Epoch 27 Train loss 0.421 Val loss 0.420 Test loss 0.421\n",
      "Epoch 28 Train loss 0.417 Val loss 0.419 Test loss 0.421\n",
      "Epoch 29 Train loss 0.419 Val loss 0.419 Test loss 0.420\n",
      "Epoch 30 Train loss 0.414 Val loss 0.413 Test loss 0.414\n",
      "Saving\n",
      "Epoch 31 Train loss 0.415 Val loss 0.419 Test loss 0.420\n",
      "Epoch 32 Train loss 0.413 Val loss 0.411 Test loss 0.412\n",
      "Saving\n",
      "Epoch 33 Train loss 0.413 Val loss 0.417 Test loss 0.418\n",
      "Epoch 34 Train loss 0.414 Val loss 0.411 Test loss 0.412\n",
      "Epoch 35 Train loss 0.410 Val loss 0.416 Test loss 0.417\n",
      "Epoch 36 Train loss 0.416 Val loss 0.419 Test loss 0.420\n",
      "Epoch 37 Train loss 0.413 Val loss 0.413 Test loss 0.414\n",
      "Epoch 38 Train loss 0.412 Val loss 0.416 Test loss 0.417\n",
      "Epoch 39 Train loss 0.415 Val loss 0.416 Test loss 0.416\n",
      "Epoch 40 Train loss 0.411 Val loss 0.413 Test loss 0.413\n",
      "Epoch 41 Train loss 0.411 Val loss 0.417 Test loss 0.417\n",
      "Epoch 42 Train loss 0.413 Val loss 0.416 Test loss 0.415\n",
      "Epoch 43 Train loss 0.410 Val loss 0.414 Test loss 0.413\n",
      "Epoch 44 Train loss 0.413 Val loss 0.416 Test loss 0.415\n",
      "Epoch 45 Train loss 0.410 Val loss 0.411 Test loss 0.410\n",
      "Epoch 46 Train loss 0.409 Val loss 0.416 Test loss 0.414\n",
      "Epoch 47 Train loss 0.413 Val loss 0.416 Test loss 0.415\n",
      "Epoch 48 Train loss 0.409 Val loss 0.412 Test loss 0.410\n",
      "Epoch 49 Train loss 0.412 Val loss 0.419 Test loss 0.418\n",
      "Epoch 50 Train loss 0.410 Val loss 0.411 Test loss 0.410\n",
      "Saving\n",
      "Epoch 51 Train loss 0.411 Val loss 0.419 Test loss 0.417\n",
      "Epoch 52 Train loss 0.415 Val loss 0.417 Test loss 0.415\n",
      "Epoch 53 Train loss 0.409 Val loss 0.412 Test loss 0.410\n",
      "Epoch 54 Train loss 0.412 Val loss 0.421 Test loss 0.420\n",
      "Epoch 55 Train loss 0.411 Val loss 0.412 Test loss 0.410\n",
      "Epoch 56 Train loss 0.409 Val loss 0.418 Test loss 0.416\n",
      "Epoch 57 Train loss 0.416 Val loss 0.421 Test loss 0.419\n",
      "Epoch 58 Train loss 0.412 Val loss 0.413 Test loss 0.411\n",
      "Epoch 59 Train loss 0.407 Val loss 0.413 Test loss 0.411\n",
      "Epoch 60 Train loss 0.411 Val loss 0.418 Test loss 0.416\n",
      "Epoch 61 Train loss 0.412 Val loss 0.413 Test loss 0.411\n",
      "Epoch 62 Train loss 0.407 Val loss 0.414 Test loss 0.412\n",
      "Epoch 63 Train loss 0.409 Val loss 0.416 Test loss 0.414\n",
      "Epoch 64 Train loss 0.411 Val loss 0.416 Test loss 0.414\n",
      "Epoch 65 Train loss 0.409 Val loss 0.412 Test loss 0.410\n",
      "Epoch 66 Train loss 0.407 Val loss 0.411 Test loss 0.410\n",
      "Saving\n",
      "Epoch 67 Train loss 0.408 Val loss 0.412 Test loss 0.411\n",
      "Epoch 68 Train loss 0.407 Val loss 0.411 Test loss 0.410\n",
      "Epoch 69 Train loss 0.406 Val loss 0.411 Test loss 0.411\n",
      "Epoch 70 Train loss 0.408 Val loss 0.415 Test loss 0.415\n",
      "Epoch 71 Train loss 0.411 Val loss 0.415 Test loss 0.415\n",
      "Epoch 72 Train loss 0.409 Val loss 0.413 Test loss 0.413\n",
      "Epoch 73 Train loss 0.407 Val loss 0.411 Test loss 0.410\n",
      "Epoch 74 Train loss 0.408 Val loss 0.413 Test loss 0.413\n",
      "Epoch 75 Train loss 0.410 Val loss 0.414 Test loss 0.413\n",
      "Epoch 76 Train loss 0.408 Val loss 0.411 Test loss 0.409\n",
      "Epoch 77 Train loss 0.406 Val loss 0.412 Test loss 0.409\n",
      "Epoch 78 Train loss 0.408 Val loss 0.416 Test loss 0.412\n",
      "Epoch 79 Train loss 0.409 Val loss 0.416 Test loss 0.411\n",
      "Epoch 80 Train loss 0.408 Val loss 0.413 Test loss 0.409\n",
      "Epoch 81 Train loss 0.407 Val loss 0.414 Test loss 0.410\n",
      "Epoch 82 Train loss 0.409 Val loss 0.414 Test loss 0.411\n",
      "Epoch 83 Train loss 0.407 Val loss 0.411 Test loss 0.410\n",
      "Epoch 84 Train loss 0.407 Val loss 0.412 Test loss 0.410\n",
      "Epoch 85 Train loss 0.409 Val loss 0.414 Test loss 0.412\n",
      "Epoch 86 Train loss 0.408 Val loss 0.412 Test loss 0.412\n",
      "Epoch 87 Train loss 0.408 Val loss 0.413 Test loss 0.412\n",
      "Epoch 88 Train loss 0.408 Val loss 0.413 Test loss 0.410\n",
      "Epoch 89 Train loss 0.407 Val loss 0.413 Test loss 0.410\n",
      "Epoch 90 Train loss 0.409 Val loss 0.414 Test loss 0.410\n",
      "Epoch 91 Train loss 0.409 Val loss 0.411 Test loss 0.407\n",
      "Saving\n",
      "Epoch 92 Train loss 0.405 Val loss 0.411 Test loss 0.410\n",
      "Epoch 93 Train loss 0.407 Val loss 0.414 Test loss 0.411\n",
      "Epoch 94 Train loss 0.410 Val loss 0.413 Test loss 0.411\n",
      "Epoch 95 Train loss 0.405 Val loss 0.411 Test loss 0.411\n",
      "Epoch 96 Train loss 0.406 Val loss 0.408 Test loss 0.406\n",
      "Saving\n",
      "Epoch 97 Train loss 0.402 Val loss 0.410 Test loss 0.409\n",
      "Epoch 98 Train loss 0.406 Val loss 0.408 Test loss 0.407\n",
      "Epoch 99 Train loss 0.408 Val loss 0.414 Test loss 0.416\n",
      "Epoch 100 Train loss 0.406 Val loss 0.406 Test loss 0.404\n",
      "Saving\n",
      "Epoch 101 Train loss 0.407 Val loss 0.413 Test loss 0.412\n",
      "Epoch 102 Train loss 0.402 Val loss 0.405 Test loss 0.405\n",
      "Saving\n",
      "Epoch 103 Train loss 0.409 Val loss 0.414 Test loss 0.417\n",
      "Epoch 104 Train loss 0.409 Val loss 0.407 Test loss 0.407\n",
      "Epoch 105 Train loss 0.405 Val loss 0.414 Test loss 0.415\n",
      "Epoch 106 Train loss 0.407 Val loss 0.407 Test loss 0.406\n",
      "Epoch 107 Train loss 0.408 Val loss 0.408 Test loss 0.410\n",
      "Epoch 108 Train loss 0.411 Val loss 0.411 Test loss 0.415\n",
      "Epoch 109 Train loss 0.411 Val loss 0.409 Test loss 0.409\n",
      "Epoch 110 Train loss 0.407 Val loss 0.412 Test loss 0.413\n",
      "Epoch 111 Train loss 0.417 Val loss 0.419 Test loss 0.419\n",
      "Epoch 112 Train loss 0.412 Val loss 0.406 Test loss 0.406\n",
      "Epoch 113 Train loss 0.404 Val loss 0.409 Test loss 0.411\n",
      "Epoch 114 Train loss 0.420 Val loss 0.424 Test loss 0.425\n",
      "Epoch 115 Train loss 0.415 Val loss 0.404 Test loss 0.405\n",
      "Saving\n",
      "Epoch 116 Train loss 0.406 Val loss 0.412 Test loss 0.412\n",
      "Epoch 117 Train loss 0.418 Val loss 0.423 Test loss 0.424\n",
      "Epoch 118 Train loss 0.419 Val loss 0.413 Test loss 0.415\n",
      "Epoch 119 Train loss 0.411 Val loss 0.411 Test loss 0.412\n",
      "Epoch 120 Train loss 0.411 Val loss 0.414 Test loss 0.415\n",
      "Epoch 121 Train loss 0.415 Val loss 0.417 Test loss 0.419\n",
      "Epoch 122 Train loss 0.415 Val loss 0.412 Test loss 0.414\n",
      "Epoch 123 Train loss 0.411 Val loss 0.410 Test loss 0.412\n",
      "Epoch 124 Train loss 0.412 Val loss 0.414 Test loss 0.414\n",
      "Epoch 125 Train loss 0.415 Val loss 0.420 Test loss 0.418\n",
      "Epoch 126 Train loss 0.418 Val loss 0.424 Test loss 0.422\n",
      "Epoch 127 Train loss 0.420 Val loss 0.426 Test loss 0.425\n",
      "Epoch 128 Train loss 0.421 Val loss 0.425 Test loss 0.425\n",
      "Epoch 129 Train loss 0.420 Val loss 0.423 Test loss 0.424\n",
      "Epoch 130 Train loss 0.420 Val loss 0.423 Test loss 0.425\n",
      "Epoch 131 Train loss 0.421 Val loss 0.424 Test loss 0.426\n",
      "Epoch 132 Train loss 0.421 Val loss 0.422 Test loss 0.423\n",
      "Epoch 133 Train loss 0.421 Val loss 0.422 Test loss 0.424\n",
      "Epoch 134 Train loss 0.425 Val loss 0.425 Test loss 0.427\n",
      "Epoch 135 Train loss 0.428 Val loss 0.425 Test loss 0.428\n",
      "Epoch 136 Train loss 0.427 Val loss 0.423 Test loss 0.426\n",
      "Epoch 137 Train loss 0.425 Val loss 0.423 Test loss 0.424\n",
      "Epoch 138 Train loss 0.423 Val loss 0.422 Test loss 0.423\n",
      "Epoch 139 Train loss 0.422 Val loss 0.420 Test loss 0.421\n",
      "Epoch 140 Train loss 0.420 Val loss 0.420 Test loss 0.423\n",
      "Epoch 141 Train loss 0.422 Val loss 0.423 Test loss 0.426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142 Train loss 0.425 Val loss 0.426 Test loss 0.429\n",
      "Epoch 143 Train loss 0.427 Val loss 0.425 Test loss 0.430\n",
      "Epoch 144 Train loss 0.427 Val loss 0.423 Test loss 0.427\n",
      "Epoch 145 Train loss 0.424 Val loss 0.421 Test loss 0.424\n",
      "Epoch 146 Train loss 0.421 Val loss 0.421 Test loss 0.422\n",
      "Epoch 147 Train loss 0.423 Val loss 0.421 Test loss 0.422\n",
      "Epoch 148 Train loss 0.422 Val loss 0.420 Test loss 0.420\n",
      "Epoch 149 Train loss 0.421 Val loss 0.421 Test loss 0.422\n",
      "Epoch 150 Train loss 0.422 Val loss 0.422 Test loss 0.423\n",
      "Epoch 151 Train loss 0.423 Val loss 0.424 Test loss 0.426\n",
      "Epoch 152 Train loss 0.425 Val loss 0.425 Test loss 0.428\n",
      "Epoch 153 Train loss 0.423 Val loss 0.423 Test loss 0.425\n",
      "Epoch 154 Train loss 0.421 Val loss 0.422 Test loss 0.423\n",
      "Epoch 155 Train loss 0.421 Val loss 0.421 Test loss 0.424\n",
      "Epoch 156 Train loss 0.423 Val loss 0.422 Test loss 0.425\n",
      "Epoch 157 Train loss 0.423 Val loss 0.424 Test loss 0.426\n",
      "Epoch 158 Train loss 0.422 Val loss 0.424 Test loss 0.428\n",
      "Epoch 159 Train loss 0.422 Val loss 0.423 Test loss 0.427\n",
      "Epoch 160 Train loss 0.420 Val loss 0.421 Test loss 0.424\n",
      "Epoch 161 Train loss 0.418 Val loss 0.419 Test loss 0.421\n",
      "Epoch 162 Train loss 0.417 Val loss 0.418 Test loss 0.419\n",
      "Epoch 163 Train loss 0.415 Val loss 0.418 Test loss 0.419\n",
      "Epoch 164 Train loss 0.417 Val loss 0.421 Test loss 0.422\n",
      "Epoch 165 Train loss 0.417 Val loss 0.418 Test loss 0.419\n",
      "Epoch 166 Train loss 0.413 Val loss 0.412 Test loss 0.414\n",
      "Epoch 167 Train loss 0.412 Val loss 0.412 Test loss 0.414\n",
      "Epoch 168 Train loss 0.412 Val loss 0.413 Test loss 0.414\n",
      "Epoch 169 Train loss 0.413 Val loss 0.414 Test loss 0.417\n",
      "Epoch 170 Train loss 0.412 Val loss 0.410 Test loss 0.412\n",
      "Epoch 171 Train loss 0.411 Val loss 0.412 Test loss 0.415\n",
      "Epoch 172 Train loss 0.412 Val loss 0.407 Test loss 0.409\n",
      "Epoch 173 Train loss 0.409 Val loss 0.411 Test loss 0.414\n",
      "Epoch 174 Train loss 0.411 Val loss 0.416 Test loss 0.416\n",
      "Epoch 175 Train loss 0.415 Val loss 0.418 Test loss 0.418\n",
      "Epoch 176 Train loss 0.413 Val loss 0.414 Test loss 0.414\n",
      "Epoch 177 Train loss 0.414 Val loss 0.413 Test loss 0.415\n",
      "Epoch 178 Train loss 0.411 Val loss 0.414 Test loss 0.414\n",
      "Epoch 179 Train loss 0.418 Val loss 0.413 Test loss 0.414\n",
      "Epoch 180 Train loss 0.411 Val loss 0.415 Test loss 0.417\n",
      "Epoch 181 Train loss 0.418 Val loss 0.415 Test loss 0.416\n",
      "Epoch 182 Train loss 0.414 Val loss 0.415 Test loss 0.417\n",
      "Epoch 183 Train loss 0.418 Val loss 0.422 Test loss 0.422\n",
      "Epoch 184 Train loss 0.424 Val loss 0.421 Test loss 0.425\n",
      "Epoch 185 Train loss 0.412 Val loss 0.410 Test loss 0.413\n",
      "Epoch 186 Train loss 0.427 Val loss 0.421 Test loss 0.423\n",
      "Epoch 187 Train loss 0.419 Val loss 0.417 Test loss 0.421\n",
      "Epoch 188 Train loss 0.421 Val loss 0.428 Test loss 0.432\n",
      "Epoch 189 Train loss 0.425 Val loss 0.423 Test loss 0.423\n",
      "Epoch 190 Train loss 0.426 Val loss 0.414 Test loss 0.417\n",
      "Epoch 191 Train loss 0.412 Val loss 0.411 Test loss 0.413\n",
      "Epoch 192 Train loss 0.422 Val loss 0.428 Test loss 0.431\n",
      "Epoch 193 Train loss 0.428 Val loss 0.420 Test loss 0.424\n",
      "Epoch 194 Train loss 0.416 Val loss 0.410 Test loss 0.414\n",
      "Epoch 195 Train loss 0.416 Val loss 0.415 Test loss 0.416\n",
      "Epoch 196 Train loss 0.422 Val loss 0.430 Test loss 0.432\n",
      "Epoch 197 Train loss 0.430 Val loss 0.424 Test loss 0.428\n",
      "Epoch 198 Train loss 0.414 Val loss 0.408 Test loss 0.409\n",
      "Epoch 199 Train loss 0.417 Val loss 0.422 Test loss 0.426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_bce = AE(x_train.shape[1], 2).to(device)\n",
    "optimizer_bce = torch.optim.Adamax(model_bce.parameters(), lr=lr, weight_decay=wd)\n",
    "loss_bce = train_my_model(epochs, model_bce, optimizer_bce, bcel, train_loader, val_loader, test_loader)\n",
    "model_bce.load_state_dict(torch.load(\"models/ae.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train loss 0.169 Val loss 0.156 Test loss 0.156\n",
      "Saving\n",
      "Epoch 1 Train loss 0.137 Val loss 0.118 Test loss 0.118\n",
      "Saving\n",
      "Epoch 2 Train loss 0.100 Val loss 0.090 Test loss 0.090\n",
      "Saving\n",
      "Epoch 3 Train loss 0.087 Val loss 0.088 Test loss 0.087\n",
      "Saving\n",
      "Epoch 4 Train loss 0.088 Val loss 0.092 Test loss 0.091\n",
      "Epoch 5 Train loss 0.091 Val loss 0.094 Test loss 0.092\n",
      "Epoch 6 Train loss 0.095 Val loss 0.094 Test loss 0.093\n",
      "Epoch 7 Train loss 0.097 Val loss 0.100 Test loss 0.101\n",
      "Epoch 8 Train loss 0.094 Val loss 0.097 Test loss 0.096\n",
      "Epoch 9 Train loss 0.098 Val loss 0.095 Test loss 0.095\n",
      "Epoch 10 Train loss 0.092 Val loss 0.093 Test loss 0.094\n",
      "Epoch 11 Train loss 0.092 Val loss 0.090 Test loss 0.090\n",
      "Epoch 12 Train loss 0.086 Val loss 0.084 Test loss 0.084\n",
      "Saving\n",
      "Epoch 13 Train loss 0.082 Val loss 0.082 Test loss 0.082\n",
      "Saving\n",
      "Epoch 14 Train loss 0.080 Val loss 0.080 Test loss 0.079\n",
      "Saving\n",
      "Epoch 15 Train loss 0.078 Val loss 0.079 Test loss 0.078\n",
      "Saving\n",
      "Epoch 16 Train loss 0.078 Val loss 0.080 Test loss 0.080\n",
      "Epoch 17 Train loss 0.079 Val loss 0.078 Test loss 0.078\n",
      "Saving\n",
      "Epoch 18 Train loss 0.076 Val loss 0.076 Test loss 0.076\n",
      "Saving\n",
      "Epoch 19 Train loss 0.074 Val loss 0.077 Test loss 0.075\n",
      "Epoch 20 Train loss 0.075 Val loss 0.078 Test loss 0.075\n",
      "Epoch 21 Train loss 0.075 Val loss 0.078 Test loss 0.075\n",
      "Epoch 22 Train loss 0.073 Val loss 0.076 Test loss 0.073\n",
      "Saving\n",
      "Epoch 23 Train loss 0.071 Val loss 0.074 Test loss 0.071\n",
      "Saving\n",
      "Epoch 24 Train loss 0.071 Val loss 0.074 Test loss 0.072\n",
      "Saving\n",
      "Epoch 25 Train loss 0.073 Val loss 0.075 Test loss 0.073\n",
      "Epoch 26 Train loss 0.073 Val loss 0.074 Test loss 0.073\n",
      "Epoch 27 Train loss 0.072 Val loss 0.072 Test loss 0.071\n",
      "Saving\n",
      "Epoch 28 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Saving\n",
      "Epoch 29 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Saving\n",
      "Epoch 30 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Saving\n",
      "Epoch 31 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Epoch 32 Train loss 0.070 Val loss 0.071 Test loss 0.071\n",
      "Epoch 33 Train loss 0.070 Val loss 0.071 Test loss 0.071\n",
      "Saving\n",
      "Epoch 34 Train loss 0.070 Val loss 0.070 Test loss 0.070\n",
      "Saving\n",
      "Epoch 35 Train loss 0.070 Val loss 0.071 Test loss 0.071\n",
      "Epoch 36 Train loss 0.070 Val loss 0.071 Test loss 0.071\n",
      "Epoch 37 Train loss 0.071 Val loss 0.072 Test loss 0.072\n",
      "Epoch 38 Train loss 0.071 Val loss 0.072 Test loss 0.072\n",
      "Epoch 39 Train loss 0.071 Val loss 0.073 Test loss 0.072\n",
      "Epoch 40 Train loss 0.072 Val loss 0.073 Test loss 0.073\n",
      "Epoch 41 Train loss 0.072 Val loss 0.072 Test loss 0.072\n",
      "Epoch 42 Train loss 0.072 Val loss 0.073 Test loss 0.072\n",
      "Epoch 43 Train loss 0.072 Val loss 0.073 Test loss 0.072\n",
      "Epoch 44 Train loss 0.072 Val loss 0.073 Test loss 0.072\n",
      "Epoch 45 Train loss 0.071 Val loss 0.072 Test loss 0.071\n",
      "Epoch 46 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Epoch 47 Train loss 0.070 Val loss 0.070 Test loss 0.070\n",
      "Saving\n",
      "Epoch 48 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 49 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 50 Train loss 0.069 Val loss 0.070 Test loss 0.069\n",
      "Epoch 51 Train loss 0.069 Val loss 0.070 Test loss 0.070\n",
      "Epoch 52 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 53 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 54 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 55 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 56 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Epoch 57 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Epoch 58 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Epoch 59 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 60 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 61 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Epoch 62 Train loss 0.069 Val loss 0.069 Test loss 0.069\n",
      "Epoch 63 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 64 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 65 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 66 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 67 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Saving\n",
      "Epoch 68 Train loss 0.068 Val loss 0.068 Test loss 0.069\n",
      "Saving\n",
      "Epoch 69 Train loss 0.067 Val loss 0.069 Test loss 0.068\n",
      "Epoch 70 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Saving\n",
      "Epoch 71 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Saving\n",
      "Epoch 72 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Saving\n",
      "Epoch 73 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Saving\n",
      "Epoch 74 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Saving\n",
      "Epoch 75 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Epoch 76 Train loss 0.067 Val loss 0.069 Test loss 0.069\n",
      "Epoch 77 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Epoch 78 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Epoch 79 Train loss 0.067 Val loss 0.069 Test loss 0.069\n",
      "Epoch 80 Train loss 0.067 Val loss 0.068 Test loss 0.067\n",
      "Epoch 81 Train loss 0.067 Val loss 0.069 Test loss 0.069\n",
      "Epoch 82 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Epoch 83 Train loss 0.067 Val loss 0.069 Test loss 0.069\n",
      "Epoch 84 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 85 Train loss 0.067 Val loss 0.068 Test loss 0.068\n",
      "Epoch 86 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 87 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 88 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 89 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 90 Train loss 0.067 Val loss 0.069 Test loss 0.069\n",
      "Epoch 91 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 92 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 93 Train loss 0.068 Val loss 0.071 Test loss 0.070\n",
      "Epoch 94 Train loss 0.069 Val loss 0.069 Test loss 0.068\n",
      "Epoch 95 Train loss 0.067 Val loss 0.070 Test loss 0.069\n",
      "Epoch 96 Train loss 0.069 Val loss 0.071 Test loss 0.070\n",
      "Epoch 97 Train loss 0.068 Val loss 0.068 Test loss 0.067\n",
      "Epoch 98 Train loss 0.067 Val loss 0.071 Test loss 0.069\n",
      "Epoch 99 Train loss 0.069 Val loss 0.069 Test loss 0.068\n",
      "Epoch 100 Train loss 0.067 Val loss 0.069 Test loss 0.067\n",
      "Epoch 101 Train loss 0.068 Val loss 0.071 Test loss 0.070\n",
      "Epoch 102 Train loss 0.067 Val loss 0.068 Test loss 0.067\n",
      "Epoch 103 Train loss 0.068 Val loss 0.071 Test loss 0.070\n",
      "Epoch 104 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Epoch 105 Train loss 0.068 Val loss 0.069 Test loss 0.068\n",
      "Epoch 106 Train loss 0.068 Val loss 0.071 Test loss 0.070\n",
      "Epoch 107 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Epoch 108 Train loss 0.068 Val loss 0.069 Test loss 0.069\n",
      "Epoch 109 Train loss 0.068 Val loss 0.073 Test loss 0.072\n",
      "Epoch 110 Train loss 0.072 Val loss 0.072 Test loss 0.071\n",
      "Epoch 111 Train loss 0.068 Val loss 0.070 Test loss 0.071\n",
      "Epoch 112 Train loss 0.070 Val loss 0.072 Test loss 0.072\n",
      "Epoch 113 Train loss 0.070 Val loss 0.075 Test loss 0.074\n",
      "Epoch 114 Train loss 0.071 Val loss 0.071 Test loss 0.070\n",
      "Epoch 115 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 116 Train loss 0.071 Val loss 0.074 Test loss 0.074\n",
      "Epoch 117 Train loss 0.073 Val loss 0.074 Test loss 0.074\n",
      "Epoch 118 Train loss 0.071 Val loss 0.072 Test loss 0.071\n",
      "Epoch 119 Train loss 0.071 Val loss 0.075 Test loss 0.074\n",
      "Epoch 120 Train loss 0.074 Val loss 0.077 Test loss 0.076\n",
      "Epoch 121 Train loss 0.072 Val loss 0.074 Test loss 0.073\n",
      "Epoch 122 Train loss 0.071 Val loss 0.073 Test loss 0.072\n",
      "Epoch 123 Train loss 0.070 Val loss 0.071 Test loss 0.070\n",
      "Epoch 124 Train loss 0.069 Val loss 0.073 Test loss 0.072\n",
      "Epoch 125 Train loss 0.071 Val loss 0.074 Test loss 0.074\n",
      "Epoch 126 Train loss 0.069 Val loss 0.072 Test loss 0.072\n",
      "Epoch 127 Train loss 0.071 Val loss 0.074 Test loss 0.074\n",
      "Epoch 128 Train loss 0.072 Val loss 0.075 Test loss 0.074\n",
      "Epoch 129 Train loss 0.072 Val loss 0.074 Test loss 0.075\n",
      "Epoch 130 Train loss 0.071 Val loss 0.072 Test loss 0.071\n",
      "Epoch 131 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 132 Train loss 0.068 Val loss 0.072 Test loss 0.071\n",
      "Epoch 133 Train loss 0.070 Val loss 0.072 Test loss 0.073\n",
      "Epoch 134 Train loss 0.069 Val loss 0.071 Test loss 0.070\n",
      "Epoch 135 Train loss 0.068 Val loss 0.072 Test loss 0.071\n",
      "Epoch 136 Train loss 0.071 Val loss 0.072 Test loss 0.071\n",
      "Epoch 137 Train loss 0.069 Val loss 0.072 Test loss 0.070\n",
      "Epoch 138 Train loss 0.069 Val loss 0.071 Test loss 0.071\n",
      "Epoch 139 Train loss 0.071 Val loss 0.073 Test loss 0.073\n",
      "Epoch 140 Train loss 0.071 Val loss 0.072 Test loss 0.071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 142 Train loss 0.068 Val loss 0.070 Test loss 0.069\n",
      "Epoch 143 Train loss 0.070 Val loss 0.074 Test loss 0.075\n",
      "Epoch 144 Train loss 0.071 Val loss 0.071 Test loss 0.071\n",
      "Epoch 145 Train loss 0.069 Val loss 0.071 Test loss 0.070\n",
      "Epoch 146 Train loss 0.068 Val loss 0.070 Test loss 0.070\n",
      "Epoch 147 Train loss 0.070 Val loss 0.072 Test loss 0.073\n",
      "Epoch 148 Train loss 0.073 Val loss 0.074 Test loss 0.076\n",
      "Epoch 149 Train loss 0.076 Val loss 0.075 Test loss 0.077\n",
      "Epoch 150 Train loss 0.075 Val loss 0.073 Test loss 0.075\n",
      "Epoch 151 Train loss 0.073 Val loss 0.071 Test loss 0.072\n",
      "Epoch 152 Train loss 0.071 Val loss 0.071 Test loss 0.071\n",
      "Epoch 153 Train loss 0.070 Val loss 0.071 Test loss 0.071\n",
      "Epoch 154 Train loss 0.070 Val loss 0.072 Test loss 0.071\n",
      "Epoch 155 Train loss 0.070 Val loss 0.072 Test loss 0.071\n",
      "Epoch 156 Train loss 0.071 Val loss 0.073 Test loss 0.072\n",
      "Epoch 157 Train loss 0.074 Val loss 0.074 Test loss 0.074\n",
      "Epoch 158 Train loss 0.072 Val loss 0.073 Test loss 0.073\n",
      "Epoch 159 Train loss 0.072 Val loss 0.073 Test loss 0.072\n",
      "Epoch 160 Train loss 0.072 Val loss 0.074 Test loss 0.073\n",
      "Epoch 161 Train loss 0.072 Val loss 0.074 Test loss 0.074\n",
      "Epoch 162 Train loss 0.073 Val loss 0.073 Test loss 0.073\n",
      "Epoch 163 Train loss 0.073 Val loss 0.074 Test loss 0.073\n",
      "Epoch 164 Train loss 0.072 Val loss 0.075 Test loss 0.075\n",
      "Epoch 165 Train loss 0.073 Val loss 0.073 Test loss 0.073\n",
      "Epoch 166 Train loss 0.074 Val loss 0.075 Test loss 0.075\n",
      "Epoch 167 Train loss 0.073 Val loss 0.075 Test loss 0.076\n",
      "Epoch 168 Train loss 0.075 Val loss 0.074 Test loss 0.075\n",
      "Epoch 169 Train loss 0.073 Val loss 0.077 Test loss 0.077\n",
      "Epoch 170 Train loss 0.075 Val loss 0.073 Test loss 0.074\n",
      "Epoch 171 Train loss 0.073 Val loss 0.076 Test loss 0.078\n",
      "Epoch 172 Train loss 0.077 Val loss 0.074 Test loss 0.075\n",
      "Epoch 173 Train loss 0.074 Val loss 0.077 Test loss 0.077\n",
      "Epoch 174 Train loss 0.079 Val loss 0.077 Test loss 0.076\n",
      "Epoch 175 Train loss 0.075 Val loss 0.076 Test loss 0.078\n",
      "Epoch 176 Train loss 0.080 Val loss 0.080 Test loss 0.083\n",
      "Epoch 177 Train loss 0.076 Val loss 0.074 Test loss 0.074\n",
      "Epoch 178 Train loss 0.076 Val loss 0.082 Test loss 0.081\n",
      "Epoch 179 Train loss 0.078 Val loss 0.076 Test loss 0.074\n",
      "Epoch 180 Train loss 0.075 Val loss 0.082 Test loss 0.081\n",
      "Epoch 181 Train loss 0.082 Val loss 0.082 Test loss 0.081\n",
      "Epoch 182 Train loss 0.075 Val loss 0.077 Test loss 0.076\n",
      "Epoch 183 Train loss 0.080 Val loss 0.084 Test loss 0.083\n",
      "Epoch 184 Train loss 0.082 Val loss 0.084 Test loss 0.083\n",
      "Epoch 185 Train loss 0.082 Val loss 0.084 Test loss 0.083\n",
      "Epoch 186 Train loss 0.082 Val loss 0.084 Test loss 0.084\n",
      "Epoch 187 Train loss 0.082 Val loss 0.084 Test loss 0.083\n",
      "Epoch 188 Train loss 0.081 Val loss 0.084 Test loss 0.083\n",
      "Epoch 189 Train loss 0.081 Val loss 0.085 Test loss 0.084\n",
      "Epoch 190 Train loss 0.083 Val loss 0.086 Test loss 0.085\n",
      "Epoch 191 Train loss 0.083 Val loss 0.084 Test loss 0.083\n",
      "Epoch 192 Train loss 0.080 Val loss 0.082 Test loss 0.082\n",
      "Epoch 193 Train loss 0.080 Val loss 0.082 Test loss 0.081\n",
      "Epoch 194 Train loss 0.080 Val loss 0.081 Test loss 0.081\n",
      "Epoch 195 Train loss 0.079 Val loss 0.081 Test loss 0.081\n",
      "Epoch 196 Train loss 0.079 Val loss 0.080 Test loss 0.080\n",
      "Epoch 197 Train loss 0.078 Val loss 0.080 Test loss 0.080\n",
      "Epoch 198 Train loss 0.078 Val loss 0.079 Test loss 0.079\n",
      "Epoch 199 Train loss 0.078 Val loss 0.079 Test loss 0.079\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mse = AE(x_train.shape[1], 2).to(device)\n",
    "optimizer_mse = torch.optim.Adamax(model_mse.parameters(), lr=lr, weight_decay=wd)\n",
    "loss_mse = train_my_model(epochs, model_mse, optimizer_mse, msel, train_loader, val_loader, test_loader)\n",
    "model_mse.load_state_dict(torch.load(\"models/ae.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_bce, x_out_bce = eval_my_model(model_bce, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_mse, x_out_mse = eval_my_model(model_mse, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f176825a0f0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6IAAAEDCAYAAAAvJ7nlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAHtpJREFUeJzt3X2UJXdZJ/DvwwQJhDATIMGskIwbDIddXAbEgIIkvK6uZ9kgZtmDuOJujBxFGFY9vrBg8A1weQkvrhiRjLK77voCQQ8v4aBJEJGNCMNrCAnLJEEIkrdJSJhAZn77x602TdM9c293V93q7s/nnDo1favqqd/t299UnnvrVlVrLQAAADCUu817AAAAAGwtGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEZ1RVe2rqlZVZ8x7LFtFVe3sfufuNbRFyR1sDrI8PMdQ5I6x0ogCAAAwKI0oAAAAg9KIAgAAMCiNKAAAAIPSiK5BVZ1UVW+qqmur6kBVfa6qXllV2w+zzf2q6qVV9fdVdXNV3V5Vn6mq/11VZ66wzbdU1fOq6q+r6saquqOqrq6qN1fVQ9fpuezpvsh+blXdo6peVFUfq6pbu8d3LFl/Z1W9vqqu6J7Drd1z+oWqOmaFfTywqn6uqt5dVVd2291SVR/pfic7ltsOFpO72XK3aNvHVNUfdhetOFBV11fVh6vqZVX1kBW2eUJVvbWqrquqr3Xzt1XVEw+zn9ZNO7vX6veq6vPd72/htbrP6n5jbCay7BjK8OTOMXRUWmumGaYk+5K0JGcn+cfu37cm+Wr375bkyiQnLrPt9yW5ftF6dyS5IcmdC48ts82JSfYu2uZgklsW/fzVJD+0Ds9rT1fv5Un+b/fvryW5ufv3jkXr/tCS53tbt+7Czx9L8oBl9vGnyzz3g4seuyrJA5fZbudKvx/T1pjkbk25qySvWLReS7J/yfPZs8x2v75o+aEkN3XzhcdetsJzWlj+77rfc+v29fVFy/4uyd3n/XdlGn6SZcdQ0/CT3DmGjnWa+wA22rQozDd3oX1c9/jduj+aL3fL37Nku1O6P9yW5CNJnpBkW7fsnkmekuTPlmxz9ySXddu8N8n3LPzhdSF/zaIwnbLG57UQ5lu7sDwzybd0y05etN/v7oL79S5k39Y9vq0b3991dS5aZh+/luRnknxHkrsteo6nL3qe71hmu50L4Zv362+azyR3a8rdz+eug9dvJzl50bITk/xkkhct2eY/LNrm9Unu3z1+vySvW7Ts2cvsb2HZTUn+MsnDusfvkeQ/JTnQLf+pef9dmYafZNkx1DT8JHeOoWOd5j6AjTYtCvNXkzx4meVPWPRH9LhFj/9x99gVSY6dcl9nd9u8Lyu885Hkjd06b1jj89qzaNxPPcx67+/W+ckVlt83yRe6dR41w/7vm8m7dIeS7FyybGccRLf0JHery12S+2dysG9JfnPKMVUm/6PSkvzRCuv8r27559L9D/GiZQvP5xNJ7rHMtq/vlv/VvP+uTMNPsuwYahp+kjvH0LFOviO6en/cWrtq6YOttYuTfKD78YeTpKruneTp3WMvaa3dOuU+fqybv7a19vUV1vmf3fwpU9Y8ko+11t6z3IKqOiXJYzN5R+33l1untXZjknfNOqZuuw9kEuDvnWXAbClyt4zD5O6Hk9wrk3dWf23KsexK8uDu37++wjov7eY7k5y2wjqvbq3dsczjF3bzh005HjYnWV6GYyg9k7tlOIbOz1HzHsAGdslhll2ayYHgkd3Pj8rkd92SvHua4lV1VO764/zdqvrtFVbd1s0fNE3dKfztYZYtHNzuneTzVbXSevdeaUxVdVqS53a1HphkuS+H/7OpRspWdMlhlsndN4/pMd384tbaV6ccy8Lv78uttU8ut0Jr7Yqq+ock39at/8FlVvu7Fer/Qzc/bsrxsDldcphlsrzCmBxDWaNLDrNM7r55TI6hPdOIrt4/TLHs+G7+gG6+v7W2f8r6903yLd2/7zfF+vecsu6RfPkwy07s5kflrud0OPda/ENV/VyS38rkHdtk8uX1mzI5bz9Jtic5OssfWCGRu1lzt7D+NTOMZeH3d7jfdZJ8PpOD6PErLF/p3fMD3dzxZ2uT5SNzDGW9yd2ROYYOaFM8iU1q8WnTj2it7R1ovwcPs2xhTB9tre2apWhV/ctMrjpWSd6Q5HeSXNFaO7honbckeXbuOsjC0DZV7tbo6AH3BettU2XZMZQNYlPlbo0cQ6fgO6Krd7hTXxaWLbxD86Vuvv1w92laYuGy7Ely0oxj68vC81jNqRTPyOTv7aLW2s+01j61+ADameadKrY2uVvdtifPsM3C7+9I+3vgkvVhFrI8G8dQ1oPcrW5bx9CeaERX7/Qpln24m38ok/stVZIfmKZ49wXvD3U/TrXNABbOwb9vVT16xm0XAveR5RZ2NxJ+zHLLYBG5m83C907OqKppT4Fa+P0d030f7ZtU1amZnFK0eH2YhSzPxjGU9SB3s3EM7ZlGdPWeWVX/fOmDVfX4TK7OlSR/kiStta8keVv32Eur6tgp97Gnmz+nqh5+uBWrqvcvLbfWPp27QvlbVXX3w4znnlV1j0UPLXy/4DtX2ORFSab9vbB1yd1sufvTTC7Xf1ySl0y5y71JFq6q+MsrrHNuN9+Xyf3iYFay7BjK8OTOMXRc5n3/mI025RtvCnxFku/tHr9bkn+byX28Wr75psAPTnJLvvGmwAs3pL5nkh9M8s4l29w9k3dyWianO/xEkvssWv6tSX4kkyudnbvG57Wn289h62RyU+CFm+m+L8njFj2PbZkcJF+S5ItZdC+zJE/NXfdG+qUk9+oePz7Jf+sev365McQ90Lb8JHery123/BcWZe8NSU5atOzEJP8lk0vzL97mmYu2eX2S+3WPL70Z948sM9aFZTtXeC7yvIUnWXYMNQ0/yZ1j6FinuQ9go02Lwnz2ouDemuT2RX88VyY5cZltn5DJFe4W1jvQHTjuXOmPKskJuetGvC2Tc+9vSPKVRY+1JL+yxuc1VZi7dX+g+4/Z0ufxtSVjOnnJdn+2aNmhJDd285bkTSuNYbOFzjT7JHdryl0lec2SdW7O5BOWhZ/3LLO/X1/y/G/s5guPvWyFcW6pg6hptkmWHUNNw09y5xg61smpuat3VSb3WHpzJn+M2zIJ+quSPKq19sWlG7TJDYMfksmV7z6RSYiPTvLZJH+U5GnLbPOPmZy3/yNJ3pnJl5oXTo/4dJI/TPLvk7x83Z7ZEbTW3pXk1ExC9uEkdyTZkcm7Zh/oxvJdrbWrl2z6zCS/mOTyJF/PJNx/k+THWmtnDzN6Nji5mzF3beKFSR6f5P9kckn5e3bbfzjJbyb5jWX291+TPCnJ2zM5WN87k/+R+PMkT26t/dL6P0u2EFl2DGV4cucYOirVddcAAAAwCJ+IAgAAMCiNKAAAAIPSiAIAADCoo+Y9ANZXVV034yavbK29spfBwBYhd7A5yDIMT+62Lo3o5vOAGde/dy+jgK1F7mBzkGUYntxtUYNeNbeqNvwlenfs2NFr/Qc96EG91v/4xz/ea32m01qreY9hOZsho8ccc0yv9Xfu3Nlr/aOO6v/9weuum/XN59l86Utf6rX+EGS0P6eeemqv9Y899tgjr7QGBw8e7LV+klx77bW91r/hhht6rT8EGe3PiSee2Gv94447rtf6d955Z6/1k+Saa67ptf6BAwd6rT+EaTLqE9EZnXHGGb3WP++883qt3/f/RLPx3e1u/X11fIg3vh72sIf1Wv+CCy7otf4JJ5zQa/0kecUrXtFr/Ve+sv8zptx6bOM6//zze61/+umn91p///79vdZPkt27d/daf8+ePb3WZ2M755xzeq3/jGc8o9f6N954Y6/1k+R5z3ter/U/9alP9Vo/SQ4dOtT7Po7ExYoAAAAYlEYUAACAQWlEAQAAGJRGFAAAgEHN1IhW1QOr6s1V9YWquqOq9lXVeVXV7+WvgCOSTxg3GYVxk1EY1tRXza2qU5J8IMkJSd6e5NNJTkvygiTfX1WPba1t/OuBwwYknzBuMgrjJqMwvFk+Ef3vmYTz+a21M1trv9hae2KS1yR5SJLf6GOAwFTkE8ZNRmHcZBQGNlUj2r1L9NQk+5L89pLFv5LktiQ/WlX93kke+CbyCeMmozBuMgrzMe0nok/o5u9prX3D3U9ba7cm+Zsk90rymHUcGzAd+YRxk1EYNxmFOZi2EX1IN//MCsuv7Oanrm04wCrIJ4ybjMK4ySjMwbQXK9rezfevsHzh8R1LF1TVOUnOmXFcwPRWnc9ERmEAMgrjJqMwB1NfNXe1WmvnJzk/Saqq9b0/YDYyCuMmozBuMgqrM+2puQvvBG1fYfnC4zevbTjAKsgnjJuMwrjJKMzBtI3oFd18pXPjv6Obr3RuPdAf+YRxk1EYNxmFOZi2Eb24mz+1qr5hm6o6Nsljk9ye5IPrODZgOvIJ4yajMG4yCnMwVSPaWvtskvck2Znkp5csfmmSY5K8pbV227qODjgi+YRxk1EYNxmF+ZjlYkU/leQDSV5XVU9KcnmSR2dy76XPJHnR+g8PmJJ8wrjJKIybjMLApj01d+Hdokcl2ZNJMH82ySlJXpvkMa21G/oYIHBk8gnjJqMwbjIKw5vp9i2ttWuT/HhPYwHWQD5h3GQUxk1GYVhTfyIKAAAA60EjCgAAwKBmOjWXZPfu3b3Wv/DCC3utD0dy6NCh3mpXVW+1FzzxiU/stf5DH/rQXutfcMEFvdZPkptuuqnX+kO8zn1qrc17CJvarl27eq1/6aWX9lp/iOP0eeed12v9IZ7DzTff3Ps+tqITTzyx9308/elP77X+VVdd1Wv9G2+8sdf6SfLiF7+41/rPetazeq0/Fj4RBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABnXUvAew0ezatavX+rt37+61PhxJVfVW+9hjj+2t9oJnPetZvda/8sore62/b9++Xusnyb3uda/e99G31tq8h7Ap7dixY95DWLO+j9MXXnhhr/WTZPv27b3W7/t3lCSXXHJJ7/vYiob47/fVV1/da/3zzz+/1/pnnXVWr/WT5Nprr+21/rZt23qtnyQHDx7sfR9H4hNRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGNVUjWlX3q6qzq+ptVXVVVX21qvZX1fur6j9XlYYW5khGYdxkFMZNRmF4096+5awkv5Pki0kuTnJNkgck+aEkb0ryA1V1VnM9fZgXGYVxk1EYNxmFgU3biH4mydOSvKO1dmjhwar65SSXJXlGJkH9s3UfITANGYVxk1EYNxmFgU11mkFr7a9aa3+xOJjd49cleWP34xnrPDZgSjIK4yajMG4yCsNbj/Pdv97N71yHWsD6k1EYNxmFcZNR6MGaGtGqOirJf+x+fPfahwOsJxmFcZNRGDcZhf5M+x3Rlbw8ycOSvLO1dtFyK1TVOUnOWeN+gNWRURg3GYVxk1Hoyaob0ap6fpKfTfLpJD+60nqttfOTnN9t40pjMBAZhXGTURg3GYV+rerU3Kp6XpLXJvlUkie01m5c11EBayKjMG4yCuMmo9C/mRvRqtqd5PVJPpFJMK9b91EBqyajMG4yCuMmozCMmRrRqvqFJK9JsjeTYP5jL6MCVkVGYdxkFMZNRmE4UzeiVfXiTL6w/fdJntRau763UQEzk1EYNxmFcZNRGNZUFyuqqh9L8qtJDib56yTPr6qlq+1rre1Z19EBU5FRGDcZhXGTURjetFfN/fZuvi3J7hXWuTTJnrUOCFgVGYVxk1EYNxmFgU11am5r7dzWWh1hOqPnsQIrkFEYNxmFcZNRGN6qbt8CAAAAq6URBQAAYFAaUQAAAAY17cWKNoTjjjuu931s37691/p79+7ttT4cyTJXCVw3Rx99dG+1Fxx77LG91v/kJz/Za/0zzjij1/pJctFFF/Vav+/XIEn279/f+z62oj7zv6Dv49zpp5/ea/3XvOY1vdZPkquvvrrX+h/96Ed7rU9/brvttt738Y53vKPX+k95ylN6rX/CCSf0Wj9JLrvssl7r3+c+9+m1fpJcf/38707kE1EAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAZ11LwHsJ6qat5DWLOdO3f2Wv/MM8/stX6S7N27t9f6l1xySa/16c+2bdt638eBAwd6rX/aaaf1Wn/Hjh291k+Sq6++utf6Rx99dK/1k+SWW27prXZrrbfaY3fTTTf1vo8zzjhjQ9d/znOe02v9JNm1a1ev9Yd4nelH38e4JHn3u9/da/3LLrus1/pnnXVWr/WT5NnPfnav9T/72c/2Wj9J3vve9/a+jyPxiSgAAACD0ogCAAAwKI0oAAAAg9KIAgAAMCiNKAAAAIPSiAIAADCoVTeiVfXsqmrddPZ6DgpYOxmFcZNRGDcZhX6tqhGtqgcleUOSr6zvcID1IKMwbjIK4yaj0L+ZG9GqqiQXJLkhyRvXfUTAmsgojJuMwrjJKAxjNZ+IPj/JE5P8eJLb1nc4wDqQURg3GYVxk1EYwEyNaFU9NMnLk7y2tfa+foYErJaMwrjJKIybjMJwpm5Eq+qoJG9Jck2SX+5tRMCqyCiMm4zCuMkoDOuoGdZ9SZJHJHlca+2r025UVeckOWfWgQEzk1EYNxmFcZNRGNBUjWhVPTqTd4Ze1Vr721l20Fo7P8n5XZ028wiBI5JRGDcZhXGTURjeEU/N7U5T+MMkn0ny4t5HBMxERmHcZBTGTUZhPqb5jui9k5ya5KFJDiy6sW9L8ivdOr/XPXZeXwMFViSjMG4yCuMmozAH05yae0eS319h2SMzOZf+/UmuSDLTqQzAupBRGDcZhXGTUZiDIzai3Ze1z15uWVWdm0k4/6C19qb1HRowDRmFcZNRGDcZhfmY6T6iAAAAsFYaUQAAAAa1pka0tXZua62cqgDjJKMwbjIK4yaj0B+fiAIAADAojSgAAACD0ogCAAAwqGnuI7phnHTSSfMewpp97nOfm/cQRu+1r31t7/vYvXt37/sYq9Zab7Wvu+663movuOaaa3qtf/zxx/dav+/xJ8kVV1zRa/3bb7+91/pJUlW91e4zA/Tvkksu6bX+3r17e62fJPv27eu1/q5du3qtnwzze9qKbrnllt73ceqpp/Za/9prr+21/qtf/epe6yfJ0572tF7rP/jBD+61fpK8973v7X0fR+ITUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABqURBQAAYFAaUQAAAAalEQUAAGBQGlEAAAAGpREFAABgUBpRAAAABnXUvAewnvbu3dv7Pvbv39/7Pvp05pln9r6P5zznOb3Wf8ELXtBr/STZvXt37/vYirZt29b7Pj74wQ/2Wv/444/vtf6BAwd6rZ8kN910U6/1Tz755F7rJ8nll1/e+z7ox65du3qt3/f/C+zYsaPX+kmyffv2Xuv3/Rokw/w/2VbUWut9H2effXav9d/1rnf1Wv+Rj3xkr/WT5P73v3+v9Y877rhe64+FT0QBAAAYlEYUAACAQWlEAQAAGJRGFAAAgEFpRAEAABjUzI1oVT2pqt5WVddV1R1V9YWquqiq/k0fAwRmI6MwbjIK4yWfMJyZbt9SVb+V5OeTfD7Jnye5PsnxSb4ryRlJ3rnO4wNmIKMwbjIK4yWfMKypG9Gq+olMwvkHSc5prX1tyfK7r/PYgBnIKIybjMJ4yScMb6pTc6vqHkl+I8k1WSacSdJa+/o6jw2YkozCuMkojJd8wnxM+4noUzI5NeG8JIeq6geTPCzJgSSXtdb+tqfxAdORURg3GYXxkk+Yg2kb0e/u5geSfCSTcP6Tqnpfkh9urX15HccGTE9GYdxkFMZLPmEOpr1q7gnd/OeTtCTfl+TYJP8qyXuSPD7Jnyy3YVWdU1UfqqoPrXGswMpkFMZNRmG8Vp3PREZhtaZtRBfWuzPJ01pr72+tfaW19vEkT8/k6mKnV9X3LN2wtXZ+a+1RrbVHrc+QgWXIKIybjMJ4rTqfiYzCak3biN7czT/SWtu3eEFr7fYkF3U/nrZO4wJmI6MwbjIK4yWfMAfTNqJXdPObV1h+Uze/59qGA6ySjMK4ySiMl3zCHEzbiP5lJufM/4uqWm6bhS91f25dRgXMSkZh3GQUxks+YQ6makRba1cn+YskJyV5weJlVfXUJP86k3eR3r3eAwSOTEZh3GQUxks+YT6mvX1Lkvx0kkckeXV3f6WPJPn2JGcmOZjk7Nba/vUfIjAlGYVxk1EYL/mEgU3diLbWPl9V35XkJUmelsmlrG/J5B2kl7XWLutniMA0ZBTGTUZhvOQThjfLJ6LpbuT7M90EjIyMwrjJKIyXfMKwpr1YEQAAAKwLjSgAAACD0ogCAAAwKI0oAAAAg5rpYkUkZ555Zq/19+zZ02v9iy++uNf6SXLppZf2Wv+FL3xhr/W3utZab7XvvPPO3mov6Ptv/MlPfnKv9a+77rpe6yfJNddc02v9yy+/vNf6SXLo0KHe90E/+j7OPfzhD++1/hD27+/3LiEXXnhhr/XpT5/H6AV9/zf8rW99a6/1Dx482Gv9JHnd617Xa/1XvepVvdYfC5+IAgAAMCiNKAAAAIPSiAIAADAojSgAAACD0ogCAAAwKI0oAAAAg9KIAgAAMCiNKAAAAIPSiAIAADAojSgAAACD0ogCAAAwKI0oAAAAg9KIAgAAMCiNKAAAAIPSiAIAADAojSgAAACD0ogCAAAwKI0oAAAAg9KIAgAAMCiNKAAAAIPSiAIAADAojSgAAACD0ogCAAAwqGqtDbezqi8nuXqGTe6f5PqehsN4bLXX+eTW2vHzHsRyZJRlbMXXWEbZSLbiayyjbCRb8TWeKqODNqKzqqoPtdYeNe9x0C+v88bltdv8vMYbm9dv8/Mab2xev83Pa7wyp+YCAAAwKI0oAAAAgxp7I3r+vAfAILzOG5fXbvPzGm9sXr/Nz2u8sXn9Nj+v8QpG/R1RAAAANp+xfyIKAADAJqMRBQAAYFCja0Sr6oFV9eaq+kJV3VFV+6rqvKo6bt5jY310r2lbYbpu3uPj8GR085PRjU1GNzf53PhkdHOT0ekdNe8BLFZVpyT5QJITkrw9yaeTnJbkBUm+v6oe21q7YY5DZP3sT3LeMo9/ZeiBMD0Z3VJkdAOS0S1DPjcoGd0yZHQKo7pYUVVdlOSpSZ7fWnv9osdfneSFSX63tfbceY2P9VFV+5KktbZzviNhVjK6NcjoxiWjm598bmwyuvnJ6PRG04h27xBdlWRfklNaa4cWLTs2yReTVJITWmu3zWWQrAsB3ZhkdOuQ0Y1JRrcG+dy4ZHRrkNHpjenU3Cd08/csDmaStNZuraq/yeQdpMck+cuhB8e6u0dVPTvJSUluS/KxJO9rrR2c77A4DBndWmR045HRrUM+NyYZ3TpkdApjakQf0s0/s8LyKzMJ56kRzs3gW5O8Zcljn6uqH2+tXTqPAXFEMrq1yOjGI6Nbh3xuTDK6dcjoFMZ01dzt3Xz/CssXHt8xwFjo1wVJnpRJSI9J8p1JfjfJziTvqqqHz29oHIaMbh0yujHJ6NYgnxuXjG4NMjqlMX0iyhbRWnvpkoc+keS5VfWVJD+b5NwkTx96XMCEjMJ4ySeMm4xOb0yfiC68C7R9heULj988wFiYjzd288fPdRSsREaR0XGT0a1NPsdPRrc2GV1iTI3oFd381BWWf0c3X+m8eja+L3fzY+Y6ClYio8jouMno1iaf4yejW5uMLjGmRvTibv7UqvqGcXWXtH5sktuTfHDogTGYx3Tz/zfXUbASGUVGx01Gtzb5HD8Z3dpkdInRNKKttc8meU8mX+T96SWLX5rJuwdvcV+lja2qHlpV3/ROUFXtTPKG7sf/MeSYmI6Mbg0yunHJ6OYnnxubjG5+Mjqbaq3Newz/pLvR7weSnJDk7UkuT/LoTO679Jkk39tau2F+I2StqurcTL6o/b4kVye5NckpSX4wydFJ3pnk6a21r81rjKxMRjc/Gd3YZHRzk8+NT0Y3Nxmdzaga0SSpqgcl+dUk35/kfkm+mORtSV7aWrtpnmNj7arq9CTPTfKI3HVZ65uT7M3kfktvaWP7o+QbyOjmJqMbn4xuXvK5Ocjo5iWjsxldIwoAAMDmNprviAIAALA1aEQBAAAYlEYUAACAQWlEAQAAGJRGFAAAgEFpRAEAABiURhQAAIBBaUQBAAAYlEYUAACAQWlEAQAAGNT/B3+rpolgwXlRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1152x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.gray()\n",
    "plt.figure()\n",
    "i1 = np.random.randint(0, x_test.shape[0])\n",
    "i2 = np.random.randint(0, x_test.shape[0])\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(4*4, 4))\n",
    "ax[0].set_title(\"bce_real\")\n",
    "ax[0].imshow(x_test[i1].cpu().numpy().reshape((8, 8)))\n",
    "ax[1].set_title(\"bce_recon\")\n",
    "ax[1].imshow(x_out_bce[i1].reshape((8, 8)))\n",
    "ax[2].set_title(\"bce_real\")\n",
    "ax[2].imshow(x_test[i2].cpu().numpy().reshape((8, 8)))\n",
    "ax[3].set_title(\"bce_recon\")\n",
    "ax[3].imshow(x_out_bce[i2].reshape((8, 8)))\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(4*4, 4))\n",
    "ax[0].set_title(\"bce_real\")\n",
    "ax[0].imshow(x_test[i1].cpu().numpy().reshape((8, 8)))\n",
    "ax[1].set_title(\"bce_recon\")\n",
    "ax[1].imshow(x_out_mse[i1].reshape((8, 8)))\n",
    "ax[2].set_title(\"bce_real\")\n",
    "ax[2].imshow(x_test[i2].cpu().numpy().reshape((8, 8)))\n",
    "ax[3].set_title(\"bce_recon\")\n",
    "ax[3].imshow(x_out_mse[i2].reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
