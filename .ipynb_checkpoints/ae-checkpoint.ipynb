{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib as mpl\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ti = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Modified by M. Romero.\n",
    "    Original: https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    #classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rc(\"font\", size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "cuda = True\n",
    "device = torch.device(\"cuda:0\" if cuda and torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "targets = digits.target\n",
    "data = digits.data\n",
    "classes = digits.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Division train-val-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 70 %\n",
      "val 10 %\n",
      "test 20 %\n"
     ]
    }
   ],
   "source": [
    "# train-validation-test split\n",
    "nsamples, ndim = data.shape\n",
    "nlabels = len(classes)\n",
    "indexes = np.arange(nsamples)\n",
    "train_idx, test_idx = train_test_split(indexes, test_size=0.2)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1/0.8)\n",
    "\n",
    "print(\"train {:.0f} %\".format(len(train_idx)/nsamples*100))\n",
    "print(\"val {:.0f} %\".format(len(val_idx)/nsamples*100))\n",
    "print(\"test {:.0f} %\".format(len(test_idx)/nsamples*100))\n",
    "\n",
    "x_train = data[train_idx]\n",
    "x_val = data[val_idx]\n",
    "x_test = data[test_idx]\n",
    "\n",
    "y_train = targets[train_idx]\n",
    "y_val = targets[val_idx]\n",
    "y_test = targets[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler() # (x - mu)/std, (mu, std) = (0, 1)\n",
    "scaler = MinMaxScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de tensores y dispositivo de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "cuda = False\n",
    "device = torch.device(\"cuda:0\" if cuda and torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, device=device, dtype=torch.float)\n",
    "x_val = torch.tensor(x_val, device=device, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test, device=device, dtype=torch.float)\n",
    "\n",
    "y_train = torch.tensor(y_train, device=device, dtype=torch.long) # ojo que al utilizar CrossEntropyLoss \n",
    "y_val = torch.tensor(y_val, device=device, dtype=torch.long)     # la entrada debe ser tipo long\n",
    "y_test = torch.tensor(y_test, device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construccion de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, ninput, nhidden, nout, bn=False, do=False):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(torch.nn.Linear(ninput, nhidden))\n",
    "        layers.append(torch.nn.BatchNorm1d(nhidden)) if bn else 0\n",
    "        layers.append(torch.nn.Dropout(0.5)) if do else 0\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(nhidden, nout))\n",
    "        self.mlp = torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    \n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, nin, nl, bn=False, do=False):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc1 = torch.nn.Linear(nin, 32)\n",
    "        self.enc2 = torch.nn.Linear(32, nl)\n",
    "        self.dec1 = torch.nn.Linear(32, nin)\n",
    "        self.dec2 = torch.nn.Linear(nl, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "    \n",
    "    def encode(self, x):\n",
    "        e1 = self.relu(self.enc1(x))\n",
    "        return self.enc2(e1)\n",
    "    \n",
    "    def decode(self, u):\n",
    "        d1 = self.relu(self.dec2(u))\n",
    "        return self.sig(self.dec1(d1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.encode(x)\n",
    "        r = self.decode(l)\n",
    "        return l, r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decalaracion de hiper parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "wd = 0.\n",
    "bs = 100\n",
    "neurons = 100\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mauricio\\anaconda3\\envs\\color\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n",
      "c:\\users\\mauricio\\anaconda3\\envs\\color\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "transform = [torchvision.transforms.ToTensor()]\n",
    "\n",
    "N = 10000\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"../datasets/mnist/train\", train=True, download=True, transform=torchvision.transforms.Compose(transform))\n",
    "testset = torchvision.datasets.MNIST(root=\"../datasets/mnist/test\", train=False, download=True, transform=torchvision.transforms.Compose(transform))\n",
    "\n",
    "train_tensor = torch.tensor(trainset.data)[:N].float().to(device).reshape(N, -1)/255.\n",
    "test_tensor = torch.tensor(testset.data)[:N].float().to(device).reshape(N, -1)/255.\n",
    "\n",
    "trainDataset = torch.utils.data.TensorDataset(train_tensor, trainset.targets[:N].to(device))\n",
    "testDataset = torch.utils.data.TensorDataset(test_tensor, testset.targets[:N].to(device))\n",
    "\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=4, shuffle=True)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaracion de dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, shuffle=True)\\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=True)\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=bs, shuffle=True)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion de perdida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cel = torch.nn.CrossEntropyLoss().to(device)\n",
    "bcel = torch.nn.BCELoss().to(device)\n",
    "msel = torch.nn.MSELoss().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funcion de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_my_model(epochs, model, optimizer, loss_function, trainloader, valloader, testloader):\n",
    "    losses = np.zeros((epochs, 3))\n",
    "    best_val_loss = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        test_loss = 0\n",
    "        model.train()\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            x_in, y_in = batch\n",
    "            _, x_out = model(x_in)\n",
    "            loss = loss_function(x_out, x_in)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= i + 1\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if valloader != None:\n",
    "                for i, batch in enumerate(valloader):\n",
    "                    x_in, y_in = batch\n",
    "                    _, x_out = model(x_in)\n",
    "                    loss = loss_function(x_out, x_in)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= i + 1\n",
    "            for i, batch in enumerate(testloader):\n",
    "                x_in, y_in = batch\n",
    "                _, x_out = model(x_in)\n",
    "                loss = loss_function(x_out, x_in)\n",
    "                test_loss += loss.item()\n",
    "            test_loss /= i + 1\n",
    "        losses[epoch] = [train_loss, val_loss, test_loss]\n",
    "        print(\"Epoch {} Train loss {:.3f} Val loss {:.3f} Test loss {:.3f}\".format(epoch, train_loss, val_loss, test_loss))\n",
    "        if valloader != None:\n",
    "            if val_loss < best_val_loss:\n",
    "                print(\"Saving\")\n",
    "                torch.save(model.state_dict(), \"models/ae.pth\")\n",
    "                best_val_loss = val_loss\n",
    "        else:\n",
    "            if test_loss < best_val_loss:\n",
    "                print(\"Saving\")\n",
    "                torch.save(model.state_dict(), \"models/ae.pth\")\n",
    "                best_val_loss = test_loss\n",
    "    return losses\n",
    "\n",
    "\n",
    "def plot_my_loss(loss, title, ylabel=\"mean cross entropy\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.plot(loss[:, 0], color=\"navy\", label=\"train\")\n",
    "    plt.plot(loss[:, 1], color=\"green\", label=\"val\")\n",
    "    plt.plot(loss[:, 2], color=\"red\", label=\"test\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 2])\n",
    "    return\n",
    "\n",
    "\n",
    "def eval_my_model(model, test_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        xl, x_out = model(test_data)\n",
    "    return xl.cpu().numpy(), x_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train loss 0.334 Val loss 0.000 Test loss 0.275\n",
      "Saving\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-95e25e118f14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_bce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moptimizer_bce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_bce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss_bce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_my_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_bce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer_bce\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbcel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel_bce\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/ae.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-119-db7afea04d9f>\u001b[0m in \u001b[0;36mtrain_my_model\u001b[1;34m(epochs, model, optimizer, loss_function, trainloader, valloader, testloader)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\mauricio\\anaconda3\\envs\\color\\lib\\site-packages\\torch\\optim\\adamax.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[1;31m# Update biased first moment estimate.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m                 \u001b[1;31m# Update the exponentially weighted infinity norm.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 norm_buf = torch.cat([\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_bce = AE(train_tensor.shape[1], 2).to(device)\n",
    "optimizer_bce = torch.optim.Adamax(model_bce.parameters(), lr=lr, weight_decay=wd)\n",
    "loss_bce = train_my_model(epochs, model_bce, optimizer_bce, bcel, trainLoader, None, testLoader)\n",
    "model_bce.load_state_dict(torch.load(\"models/ae.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mse = AE(train_tensor.shape[1], 2).to(device)\n",
    "optimizer_mse = torch.optim.Adamax(model_mse.parameters(), lr=lr, weight_decay=wd)\n",
    "loss_mse = train_my_model(epochs, model_mse, optimizer_mse, msel, trainLoader, None, testLoader)\n",
    "model_mse.load_state_dict(torch.load(\"models/ae.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl_bce, x_out_bce = eval_my_model(model_bce, test_tensor)\n",
    "xl_mse, x_out_mse = eval_my_model(model_mse, test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.gray()\n",
    "plt.figure()\n",
    "rs = 28\n",
    "i1 = np.random.randint(0, test_tensor.shape[0])\n",
    "i2 = np.random.randint(0, test_tensor.shape[0])\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(4*4, 4))\n",
    "ax[0].set_title(\"real\")\n",
    "ax[0].imshow(test_tensor[i1].cpu().numpy().reshape((rs, rs)))\n",
    "ax[1].set_title(\"bce_recon\")\n",
    "ax[1].imshow(x_out_bce[i1].reshape((rs, rs)))\n",
    "ax[2].set_title(\"real\")\n",
    "ax[2].imshow(test_tensor[i2].cpu().numpy().reshape((rs,rs)))\n",
    "ax[3].set_title(\"bce_recon\")\n",
    "ax[3].imshow(x_out_bce[i2].reshape((rs, rs)))\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(4*4, 4))\n",
    "ax[0].set_title(\"real\")\n",
    "ax[0].imshow(test_tensor[i1].cpu().numpy().reshape((rs, rs)))\n",
    "ax[1].set_title(\"mse_recon\")\n",
    "ax[1].imshow(x_out_mse[i1].reshape((rs, rs)))\n",
    "ax[2].set_title(\"real\")\n",
    "ax[2].imshow(test_tensor[i2].cpu().numpy().reshape((rs, rs)))\n",
    "ax[3].set_title(\"mse_recon\")\n",
    "ax[3].imshow(x_out_mse[i2].reshape((rs, rs)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
